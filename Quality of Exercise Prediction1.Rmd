```{r echo = FALSE , message=FALSE, warning=FALSE}
setwd("C:/Users/ADMIN/Documents/workfromhome")


fitness <- read.csv("pml-training.csv",header = TRUE, sep = ",")

fitness_test_data <- read.csv("pml-testing.csv",header = TRUE, sep = ",")

library(caret)
```

### *PREDICTION OF ACCURACY OF WORK-OUTS USING DATA GENERATED BY HUMAN ACTIVITY RECOGNITION DEVICES.*

*Credits to the research team (see http://groupware.les.inf.puc-rio.br/har) who have carried out a significant research in the area of HAR and are generous enough to allow their data to be used for this assignment.*

####  *DATA PRE-PROCESSING* 

*nearZeroVar() was used to eliminate insignificant variables and removing Nominal variable and timestamps. Also variables containing NAs in more than half of the data were removed.*

``` {r  warning=FALSE}
nzv <- nearZeroVar(fitness,saveMetrics = FALSE)

fitness_preproc <- fitness[,-nzv]

retain_col <- !names(fitness_preproc) %in% c("X","user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp","new_window","num_window")

fitness_preproc <- fitness_preproc[,retain_col]

fitness_preproc <- fitness_preproc[,colSums(is.na(fitness_preproc))<nrow(fitness_preproc)/2]

```

*The training data was split retaining 60% as training set*

```{r}
set.seed(250)
fitness_split <- createDataPartition(fitness_preproc$classe,p=0.60,list = FALSE)
```

```{r echo=FALSE}
fitness_tr <- fitness_preproc[fitness_split,]

fitness_te <- fitness_preproc[-fitness_split,]
```

####  *PRINCIPAL COMPONENT ANLAYSIS*

*There are 52 predictor variables left of which not all them may be absolutely necessary given that the data out of accelerometers or gyro or magnetometers may be correlated with each other. These variables may be significantly reduced through PCA.*

```{r echo=FALSE}
fitness_pca <- princomp(fitness_tr[,-53],scores=TRUE, cor = TRUE)
```


```{r}
screeplot(fitness_pca,type = "line",main="Fitness Scree",npcs = 52)
biplot(fitness_pca)
```

*The scree plot shows a elbow deviation at about 20th component where the cumulative variance is about 40%. The number of variables have been brought down from 52 to 20.*

#### *Fitting a rpart tree*

```{r echo=FALSE , warning=FALSE,message=FALSE}

library(rattle)

library(rpart.plot)
```

*20 Principal components were used to build a rpart tree.*

```{r warning=FALSE, message=FALSE}
fitness_pca$scores <- as.data.frame(fitness_pca$scores)

fitness_tr_pca_df <- data.frame(cbind(fitness_pca$scores[,1:20],fitness_tr$classe))

pca_tree_model <- rpart(fitness_tr_pca_df$fitness_tr.classe~. , data = fitness_tr_pca_df)

fancyRpartPlot(pca_tree_model, uniform = TRUE ,main = "Classe Classification on Training Set")
```

*The Principal Components are applied on the validation set before using the predict() on the validation set.*
```{r}

fitness_te_pca_df <- data.frame(predict(fitness_pca,fitness_te))[,1:20]

pca_tree_pred <- predict(pca_tree_model,newdata = fitness_te_pca_df, type="class")

```

```{r}
confusionMatrix(fitness_te$classe, pca_tree_pred)$overall["Accuracy"]
```

*Fitting a tree on the validation set using 20 Principal component variables have a resultant accuracy of only about 48%.*

#### *Fitting using Multinomial Logistic Regression*


```{r warning=FALSE,message=FALSE,echo=FALSE}
library(VGAM)
```

``` {r}
pca_logit_model <- vglm(fitness_tr_pca_df$fitness_tr.classe~. , data = fitness_tr_pca_df,family = "multinomial")

pca_logit_pred <- predict(pca_logit_model, fitness_te_pca_df,type="response")

fitness_te$pca_logit_pred_classe <- apply(pca_logit_pred,1,which.max)

fitness_te$pca_logit_pred_classe[which(fitness_te$pca_logit_pred_classe=="1")] <- levels(fitness_te$classe)[1]
fitness_te$pca_logit_pred_classe[which(fitness_te$pca_logit_pred_classe=="2")] <- levels(fitness_te$classe)[2]
fitness_te$pca_logit_pred_classe[which(fitness_te$pca_logit_pred_classe=="3")] <- levels(fitness_te$classe)[3]
fitness_te$pca_logit_pred_classe[which(fitness_te$pca_logit_pred_classe=="4")] <- levels(fitness_te$classe)[4]
fitness_te$pca_logit_pred_classe[which(fitness_te$pca_logit_pred_classe=="5")] <- levels(fitness_te$classe)[5]

```

*The accuracy doesn't show a big improvement.*

``` {r}

confusionMatrix(table(fitness_te$pca_logit_pred_classe,fitness_te$classe))$overall["Accuracy"]

```

#### *Fittin using Gradient Boost Method*

*Lower accuracy resulted from rpart and Logit could be because of the weak predictors. Boosting would be an appropriate to increase the accuracy*

```{r echo=FALSE,warning=FALSE,message=FALSE}

library(MASS)
```

```{r warning=FALSE,message=FALSE}
set.seed(110)


gbm_tune <- expand.grid(n.trees = 1:50,interaction.depth = 2:5,shrinkage = 0.1, n.minobsinnode = 10)

fitness_gbm <- train(fitness_tr_pca_df$fitness_tr.classe~., data = fitness_tr_pca_df, method = "gbm" , verbose=FALSE, tuneGrid = gbm_tune)

```

```{r echo=FALSE}
fitness_gbm
```

```{r warning=FALSE,message=FALSE}
fitness_pca_gbm_fit <- predict(fitness_gbm,fitness_te_pca_df,type="prob")

fitness_te$pca_gbm_pred_class <- apply(fitness_pca_gbm_fit,1,which.max)
fitness_te$pca_gbm_pred_class[which(fitness_te$pca_gbm_pred_class=="1")] <- levels(fitness_te$classe)[1]
fitness_te$pca_gbm_pred_class[which(fitness_te$pca_gbm_pred_class=="2")] <- levels(fitness_te$classe)[2]
fitness_te$pca_gbm_pred_class[which(fitness_te$pca_gbm_pred_class=="3")] <- levels(fitness_te$classe)[3]
fitness_te$pca_gbm_pred_class[which(fitness_te$pca_gbm_pred_class=="4")] <- levels(fitness_te$classe)[4]
fitness_te$pca_gbm_pred_class[which(fitness_te$pca_gbm_pred_class=="5")] <- levels(fitness_te$classe)[5]

```

```{R}

confusionMatrix(table(fitness_te$pca_gbm_pred_class,fitness_te$classe))$overall["Accuracy"]
```

*The Gradient Boost Method has resulted in a higher accuracy. It would be interesting to look if combing predictors would further increase the accuracy*

#### *Combining Predictors*


```{r}
combined_pred_logit_tree_gbm <- data.frame(fitness_te$pca_logit_pred_classe,pca_tree_pred,fitness_te$pca_gbm_pred_class,fitness_te$classe)

combined_pred_model <- train(combined_pred_logit_tree_gbm$fitness_te.classe~.,method="lda",data=combined_pred_logit_tree_gbm)

combined_pred <- predict(combined_pred_model,combined_pred_logit_tree_gbm)
```

```{r}
confusionMatrix(table(combined_pred,fitness_te$classe))$overall["Accuracy"]
```

*The combined accuracy is not beyond what was seen as a result of GBM.*

#### *Fitting using Random Forest*

*Random Forest was attempted to check if that could result in a further high accurate prediction.*

```{r echo=FALSE,warning=FALSE,message=FALSE}
library(randomForest)
```
```{r warning=FALSE,message=FALSE}
set.seed(130)

fitness_rf <- randomForest(fitness_tr$classe~., data = fitness_tr,ntree = 50)
```

*Random forest was built taking into consideration all the variables.*

```{r}
fitness_rf_pred <- predict(fitness_rf, fitness_te, type = "class")

confusionMatrix(fitness_te$classe, fitness_rf_pred)$overall["Accuracy"]

table(fitness_te$classe,fitness_rf_pred)
```

*Random Forest has resulted in the highest prediction accuracy. That would be used to predict the unknown.*

####*FINAL PREDICTION*

```{r}


test_final_pred <- predict(fitness_rf, fitness_test_data,type="class")

test_final_pred

```